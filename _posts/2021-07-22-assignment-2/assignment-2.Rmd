---
title: "Visual Assignment"
description: |
  A short description of the post.
author:
  - name: Rhoda Tong
    url: https://public.tableau.com/app/profile/mt.tong/viz/DataVizMakeover2_16240837165630/TradeDB
date: 07-22-2021
output:
  distill::distill_article:
    self_contained: false
draft: true
toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r Installing Loading Reqd Libraries, include=FALSE}

packages = c('DT','tidyverse', 'ggiraph', 'plotly', 'lubridate', 'naniar','stringr','dplyr','ggstance','ggpubr','hms','data.table','raster','sf','tmap','clock','rgdal','tibbletime','scales','knitr')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

# Background

Since 20 years ago, GASTech has been operating a natural gas production site in the island country of Kronos. The business has been profitable, and the company has also developed close relationships with the Kronos Government. In January 2014, following GASTech's initial public offering listing, several GASTech employees has gone missing. An organization known as Protectors of Kronos (POK) is suspected in these cases of missing persons, as GASTech's business moves had not been too environmentally friendly. A thorough investigation is to be carried out by the law enforcers of Kronos and Tethys to break this case.

Information and data pertaining to the whereabouts of company cars, purchases made by employees in local stores have been provided to the law enforcers. We shall use visual analytics to sense-make this data to facilitate the investigation. 

This would be done as a sub-component which would eventually feed into an interactive Shiny app for the use of the law enforcers, together with other sub-components covered by my project group mates. The objective of this assignment is to explore what the insights are and how they can be brought out from the depths of this dataset to aid in the investigation.  

# Literature Review

## Crime Analysis - Overall

Crime analysis is a law enforcement function which involves systematic analysis for identifying and analyzing patterns and trends in crime and disorder [**Wikipedia**]. Too little data would inevitably limit the efficiency of the investigation, but overwhelming volume of information could pose a huge challenge as well. Coupled with the need for rapid analysis, too much information to absorb, categorize, remember and draw meaning from could compromise the overall investigation [**Crime-Analysis ScienceDirect**]. Visual analytics techniques could be employed to gain useful insight from massive raw data.

For efficiency in data processing, information must first be consistent as subtle differences can greatly increase variability and reduce the reliability and value of a dataset [**Crime-Analysis ScienceDirect**]. Next, there cannot be information overload within a diagram. Good practices such as appropriate brushing and linking, selecting and marking, aggregation, elimination, virtual navigation techniques such as zooming, focus + context, and details-on-demand techniques have been studied and used to overcome an over-cluttered screen.[**Ku et. al, 2016**]. Uninteresting and expected patterns can also be unmarked to improve efficiency and reduce false positives. [**Arxiv**]

## Approaches to Analyze Anomalous User Behaviors

According to [**Arxiv, Visual of Anomalous User Behavior**], detection of anomalous user behaviors can be a challenging task as the boundary between abnormal and normal data may not be clearly defined, and approaches like machine learning lack contextual information to support decision-making. Visualization techniques like sequence visualization, graph visualization, text visualization, geographic visualization, chart visualization can be combined with interaction methods like tracking and monitoring, pattern discovery, exploration and navigation to analyze anomalous user behaviors.

Analysis of anomalous travel behaviors can take the following approaches:

* Difference from expected patterns indicated by historic records
* Detection of irregular driving direction, hotspots
* Characteristic travel patterns associated with groups of travelers
* Statistical methods to identify outliers such as use of boxplots to look at deviation and its extent

Analysis of anomalous transactions can take the following approaches:

* To be used in conjunction with spatiotemporal data
* Probe into time series along with details on amounts transacted, no. of transactions within a period
* Identify activities that are new to the user

## Visualizing Movement / Geospatial Map Types

According to Robert Krueger [**Year**], movement data is more complex to handle than simple point-based data as it contains complex hierarchical structures of overlapping trajectories with diverse shapes and directions. 

1. Graphs (**R. Krueger**)

Movements can be spatially aggregated. A full spatial and temporal aggregation of the trajectories can result in a static graph G = (V,E) consisting of nodes V and edges E. Each edge e = (u,v) can encode directions and contain a weight that holds the travel volumes between the nodes. Analysis is flexible with this graph network and techniques such as clustering, segmentation, aggregation can be performed.  2. Thematic Maps

![](images/JohnSnow.png)

This is the world famous cholera map produced by [Doctor John Snow](https://www.theguardian.com/news/datablog/2013/mar/15/john-snow-cholera-map) in 1854. Each bar plotted onto the map represents a death case. It was then immediately apparent where the deaths clustered, enabling investigation to be directed and focused. It eventually led them to the culprit water pump in Broad street which was polluted by sewage water tainted by a disposed baby nappy with cholera. Death statistics on their own might not have led to this discovery this soon had it not been geo-localized. This spatial autocorrelation is powerful.

3. Spatial-Temporal Perspectives: Multiple Coordinated Views (MCV)

Visualizations can include many types such as thematic maps, scatter plots, parallel coordinate plots, timelines and a wide range of other techniques. Interactivity to allow quick switching between these views can facilitate more insights. These systems are described generally as [coordinated-view visualizations](https://gistbok.ucgis.org/bok-topics/geovisual-analytics). An example is shown as below. Global population trends by country are compared using a parallel coordinate plot, choropleth map and treemap. (Image courtesy of the National Center for Visual Analytics at Linkoping University)

![](images/CoordinatedViewViz.png)

4. Spatial-Temporal Perspectives: 3D Coordinate System (Space-Time Cubes)

There can be perspective distortion and occlusion, but the spatio-temporal distribution can be highlighted. [**R. Krueger**]. [Space-time cubes](https://gisgeography.com/space-time-cubes/) show change over time within geographic space. Each cube represents a slice of time, in which the topmost cube has the newest timestamp. Temporal changes in that geographic area can then be visualized. Map below shows a space-time cube web scene in [ArcGIS Online (AGOL)](https://gisgeography.com/space-time-cubes/).

![](images/SpaceTimeCube.png)

## R Packages to Be Applied

1. DT - To make data tables interactive
2. plotly - For interactive charts
3. tmap, sf, raster - to handle geospatial data and movement visualization
4. lubridate, clock - to handle time and date values
5. scales - To handle label formatting in charts
6. naniar - For data exploration ; Check for missing values

# Data Available

The following information is available.

1. Employee that the company car is assigned to : Last Name, First Name, CarID, Current Employment Type, Current Employment Title [_car-assignments.csv_]
2. GPS Tracking Info of Company Cars : Date, Time, Car ID, Latitude, Longitude [_gps.csv_]
3. Credit Card Transactions: Location, Date, Time, Price, Credit Card No. [_cc_data.csv_]
4. Loyalty Card: Location, Date, Price, Loyalty Card No. [_loyalty_data.csv_]
5. Employee Information: LastName, FirstName, Gender, CurrentEmployment Title [EmployeeRecords.csv]

# Data Wrangling and Preparation

The type of data required would vary with the questions. This section would only cover the main data set up, EDA, quality of data, any general manipulations such as correcting the format of the data.

Special data manipulation specific to the questions would be covered in their respective sections instead.

We first load the datasets, via the read_csv() function.

```{r Loading the Datasets, echo=TRUE}

car_assigned <- read_csv("data/car-assignments.csv")
gpstracking <- read_csv("data/gps.csv")
loyaltycard <- read_csv("data/loyalty_data.csv")
creditcard <- read_csv("data/cc_data.csv")
emprecords <- read_csv("data/EmployeeRecords.csv")

```

To check for missing values, we use the _naniar_ package. The [naniar](https://naniar.njtierney.com/) package provides tidy ways to summarize, visualize and manipulate missing data.

```{r check missing values, echo=TRUE}

#For car_assigned
miss_var_summary(car_assigned)
#For creditcard
miss_var_summary(creditcard)
#For gpstracking
miss_var_summary(gpstracking)
#For loyaltycard
miss_var_summary(loyaltycard)

```

Here, we observe that the only dataset with missing values is _car_assigned_. From the filter below, we see that the truck drivers are not assigned cars. For now, other than to acknowledge this fact, we are indifferent to the missing values as there seems to be no other dataset which contains _LastName_ and _FirstName_ to be able to use the information below. Later, we would see that there is location tracking information on vehicles not within the cars list. These vehicles can be associated with any of the truck drivers below. 

For now, these rows can also be removed since we will be doing a join with the other datasets and would require a uniquely valid column without missing values. They will be removed via the _complete.cases()_ function.

```{r car_assigned missing values, echo=TRUE}

car_assigned %>% 
  filter(is.na(CarID))

car_assigned_only <- car_assigned[complete.cases(car_assigned),]

```

Next, we shall ensure that the Timestamps are in the right and consistent format, using the _lubridate_ package. For the _loyaltycard_ dataset, the timestamp is only in _mdy_ format. We will observe that the timestamp will be converted to POSIXct (for creditcard and gpstracking dataset), and Date (for loyaltycard as there is only date data).


```{r formatting timestamp, echo=TRUE}

#For creditcard dataset
creditcard$TimeStampFormatted <-mdy_hm(creditcard$timestamp)

#Delete timestamp column
creditcard <- creditcard %>% 
  dplyr::select(-timestamp)

#Reorder columns
col_order <- c("TimeStampFormatted", "location","price","last4ccnum")
creditcard <- creditcard[, col_order]

#For gpstracking dataset
gpstracking$TimeStampFormatted <-mdy_hms(gpstracking$Timestamp)

#Delete timestamp column
gpstracking <- gpstracking %>% 
  dplyr::select(-Timestamp)

#Reorder columns
col_order <- c("TimeStampFormatted", "id","lat","long")
gpstracking <- gpstracking[, col_order]

#For loyaltycard dataset
loyaltycard$TimeStampFormatted <- mdy(loyaltycard$timestamp)

#Delete timestamp column
loyaltycard <- loyaltycard %>% 
  dplyr::select(-timestamp)

#Reorder columns
col_order <- c("TimeStampFormatted", "location","price","loyaltynum")
loyaltycard <- loyaltycard[, col_order]

```

We also observe special unidentifiable characters in _Katerina's Cafe_ in the _creditcard_ and _loyaltycard_ dataset. Those shall be identified and replaced using the str_replace_all() function to prevent error in data processing.

```{r special char in katerinas cafe, echo=TRUE}

#creditcard

creditcard <- creditcard %>% 
  mutate(location = str_replace_all(location,pattern = "Katerin.+",replacement = "Katerinas Cafe"))%>%   mutate(location = str_replace_all(location,pattern = "[^[:alnum:]]",replacement = " "))

#loyaltycard

loyaltycard <- loyaltycard %>% 
  mutate(location = str_replace_all(location,pattern = "Katerin.+",replacement = "Katerinas Cafe"))%>%   mutate(location = str_replace_all(location,pattern = "[^[:alnum:]]",replacement = " "))


```

Next, we include into the _gpstracking_ dataset, the first and last names of the personnel the car is assigned to. We essentially want to do a left join for the _gpstracking_ dataset, and the _car_assigned_ dataset, by the car ID. We can do this with the left_join() function. 

```{r Joining gpstracking and car_assigned}

gpsname <- left_join(gpstracking, car_assigned_only, by = c("id" = "CarID"))

#Join First Name and Last Name

gpsname$name <- paste(gpsname$FirstName, gpsname$LastName)

#Reorder cols
col_order <- c("TimeStampFormatted", "id","lat","long","name","LastName","FirstName","CurrentEmploymentType","CurrentEmploymentTitle")
gpsname <- gpsname[, col_order]

```

Next, we shall prepare the geospatial map for viewing. We would use the _Raster_ package to import the raster file for the map of Abila.

The file to be imported is already georeferenced using qGIS, into .tif format.
Note that the raster layer is a three bands false colour image, we would use tm_rgb() instead of tm_raster() to be able to display all three bands. If not, the layer would come out in monochrome.

```{r Geomap, echo=TRUE}

#Importing the Raster file

bgmap <- raster("data/MC2/MC2-tourist_modified.tif")

#Plotting the Raster Layer and defining as base layer.

tmain <- tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1, g = 2, b = 3,
            alpha = NA,
         saturation = 1,
         interpolate = TRUE,
         max.value = 255)

tmap_mode("plot")

tmain

```

Then, we shall map the aspatial data next. Essentially, the following general steps are required to be able to create layers to visualize on the map:

1. Select the latitude and longitude coordinates that we want displayed. These coordinates are in the _.dbl_ format.

2. Convert it to Simple Feature Data Frame via the _st_as_sf()_ function of the _sf_ package; Coordinates would be converted to geometry format. They would be input as longitude ('long' ; x-coordinates) and latitude ('lat' ; y-coordinates), in the EPSG: 4326 format, which is the wgs84 Geographic Coordinate System.

Example:

```{r Conversion into Simple Feature Data Frame, eval=FALSE, echo=TRUE}

gps_sf <- st_as_sf(gpstracking,
                   coords = c("long", "lat"),
                   crs = 4326)
```

3. We can either use them as individual geometry points, or string them up to form a path

4. To form a path, we use the _st_cast("LINESTRING")_ function

Example:

(Here, we are creating the movement path from GPS points for each car. Hence, we need to group the data by the car ID, the identifier. As R requires a command following the group_by() function, an input will be required for the code to run, so we include a dummy summarize() code to overcome this issue.)

```{r Creating Movement Path, eval=FALSE, echo=TRUE}

gps_path <- gps_sf %>% 
  group_by(id) %>% 
  summarize(m = mean(TimeStampFormatted),
            do_union = FALSE) %>% 
  st_cast("LINESTRING")

```

We would like to find out where the vehicles have gone to.
First, we have to identify the coordinate points where the vehicles have possibly made stops, and be able to visualize where these stops are on the map. We could decipher this information from the _gpstracking_ data, which tracks the coordinate points of vehicles as long as they are moving. This means that when there is a long gap in the timestamp at a particular coordinate point, it is likely that the vehicle has parked. Stops at traffic light junctions should take no longer than 2 minutes. As such, we shall assume for vehicles which are stationary for more than 4 minutes to be parked (i.e. search for timestamps with lag of more than 4 minutes). 

In addition, we note that for position coordinates, the number of decimal places required for a particular accuracy at the equator is:

![](images/DegreeAccuracy.jpg){width=35%}

Considering the sizes of typical carparks, the accuracy of 1.11m would be too precise. A more likely range could be 11.1m. With this, we shall round all our coordinate points up to 4 decimal points for analysis.

A map with all stops identified from the entire _gpstracking_ dataset is as shown.

```{r interactive map viewing of the points}

#Round lat long points to 4 decimal points

gpstracking$lat <- round(gpstracking$lat, 4)
gpstracking$long <- round(gpstracking$long, 4)

#Identifying all stops for all Car IDs and Time

tmap_mode("view")

all_stops <- gpstracking %>%
  group_by(id) %>%
  mutate(stop = TimeStampFormatted - lag(TimeStampFormatted)) %>%
  mutate(parked = ifelse(stop >60*4, TRUE,FALSE)) %>%
  ungroup() %>%
  filter(parked == TRUE) %>%
  distinct(lat, long)

#Converting it to sf

all_stops_sf <- st_as_sf(all_stops,
                         coords = c("long", "lat"),
                         crs = 4326) %>% 
  mutate(coordinates = geometry)

#Viewing it on the map

tm_all_stops <- tmain +
  tm_shape(all_stops_sf) +
  tm_dots(size = 0.1,
          alpha = 0.3,
          col = "red")

tm_all_stops

```

With all of the above codes, the following function _print_routes_ID_date(emp_id, start_dt,end_dt) is created to generate the routes of a specific vehicle, during a specific time period. This way, we are able to print the routes for a specific vehicle ID, for a selected time period from *start_dt* to *end_dt*. 

```{r function for printing vehicle routes by ID and datetime}

print_routes_ID_date <- function(emp_id,start_dt,end_dt){
  
  #filter gps_path by ID and datetime
  
  #Start with gpstracking, filter
  
  id_time_select <- gpstracking %>% as_tbl_time(index=TimeStampFormatted) %>%
  filter(id == emp_id) %>% 
  filter_time(start_dt ~ end_dt)
  
  #Convert to selected sf
  
  gps_sf_selected <- st_as_sf(id_time_select,
                         coords = c("long", "lat"),
                         crs = 4326) %>% 
    mutate(coordinates = geometry)
  
  #Create a LineString
  
    gps_path_selected <- gps_sf_selected %>% 
  group_by(id) %>% 
  summarize(m = mean(TimeStampFormatted),
            do_union = FALSE) %>% 
  st_cast("LINESTRING")
  
    #filter stop points by ID and datetime
  
  stops <- gpstracking %>% as_tbl_time(index=TimeStampFormatted) %>%
  group_by(id) %>%
  filter(id == emp_id) %>% 
  filter_time(start_dt ~ end_dt) %>% 
  mutate(stop = TimeStampFormatted - lag(TimeStampFormatted)) %>%
  mutate(parked = ifelse(stop >60*4, TRUE,FALSE)) %>%
  ungroup() %>%
  filter(parked == TRUE) %>%
  distinct(lat, long)
    
  #Converting stop points to sf
  
  stops_sf <- st_as_sf(stops,
                         coords = c("long", "lat"),
                         crs = 4326) 
  
  #Viewing it on the map
  
  mapviz <- tm_all_stops +
    tm_shape(gps_path_selected) +
    tm_lines() +
    tm_shape(stops_sf) +
    tm_dots(size = 0.1,
            alpha = 0.3,
            col = "green")
  
  return(mapviz)
  
}

```

Example of printing the route for ID #15, on 7th Jan.

```{r trying out the print_route function, echo=TRUE}

print_routes_ID_date(15,'2014-01-07','2014-01-07')

```

# Questions

## Question 1

**Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies?**

```{r location popularity by cc txns, echo = FALSE}

#We see the frequency of visits by the number of transactions in each location

#Via creditcard dataset

#To count occurrence by location
loccountA <- creditcard %>% group_by(location) %>% mutate(count_occur = n())

p <- ggplot(loccountA, aes(x = reorder(location, -count_occur), tooltip = location, data_id = location)) +
    geom_bar_interactive(fill="darkblue") +
    scale_x_discrete(guide = guide_axis(angle = 90)) +
    ylim(0,220) +
    geom_text(stat = 'count',aes(label=after_stat(count)), 
            position = position_dodge(width = 0.9), 
            size=6, vjust = -1) +
  ggtitle("Frequency of Credit Card Transactions in each Location") +
  theme(axis.text.x = element_text(size = 18),
        axis.text.y = element_text(size = 18),
        plot.title = element_text(size = 24, face = "bold", hjust = 0.5),
        axis.title.y = element_text(size = 20))

girafe(
  ggobj = p,
  width_svg = 15,
  height_svg = 15*0.618
)


```

From the bar plot above, by credit card transactions, **Katerina's Cafe, Hippokampos, Guy's Gyros and Brew've Been Served** are the most popular locations. We shall see the outcome when the loyalty card transactions are analyzed in the same manner. From the bar plot below, the same results are derived for the Top 4 most popular locations.

```{r location popularity by loyaltycard txns, echo = FALSE}

#We see the frequency of visits by the number of transactions in each location

#Via loyaltycard dataset

#To get the count occurrence by location

loccountB <- loyaltycard %>% group_by(location) %>% mutate(count_occur = n())

p1 <- ggplot(loccountB, aes(x = reorder(location, -count_occur), tooltip = location, data_id = location)) +
  geom_bar_interactive(fill = "darkblue") +
    scale_x_discrete(guide = guide_axis(angle = 90)) +
    ylim(0,220) +
    geom_text(stat = 'count',aes(label=after_stat(count)), 
            position = position_dodge(width = 0.9), 
            size=6, vjust = -1) +
  ggtitle("Frequency of Loyalty Card Transactions in each Location") +
  theme(axis.text.x = element_text(size = 18),
        axis.text.y = element_text(size = 18),
        plot.title = element_text(size = 24, face = "bold", hjust = 0.5),
        axis.title.y = element_text(size = 20))

girafe(
  ggobj = p1,
  width_svg = 15,
  height_svg = 15*0.618
)

```

However, the bar plots only tell us the total number of visits over the 2 weeks. We do not know which days have higher visits, or the profile of the visits over the day. To view this, we can use violin plots. The thickness of the plots will also reflect the frequency of the visit at that point in time. We will also be able to see which days did the visits take place. 

We shall use the violin plots in the _plotly_ package as they are interactive, we can hover over the plots, especially where they are thicker to see what time periods are those.

```{r for time of the day, echo=FALSE, fig.height=5, layout="1-body-outset"}

#from creditcardB

#ggplot(creditcardB, aes(TimeStampFormatted,location)) +
  #geom_violin(scale = "area")
  #scale_x_discrete(guide = guide_axis(angle = 90))

fig <- creditcard %>%
    plot_ly(
    x = ~location,
    y = ~TimeStampFormatted,
    type = 'violin',
    split = ~location,
    box = list(
      visible = T
    ),
    meanline = list(
      visible = T
        )
  )

fig <- fig %>% 
  layout(title = 'Violin Plots of Spread of Visits Across the 2 Weeks for Each Location',
    xaxis = list(
      title = "Location"
    ),
    yaxis = list(
      title = "Timestamp",
      zeroline = F,
      range = c(as.numeric(as.POSIXct("1/4/2014", format="%m/%d/%Y"))*1000,
                as.numeric(as.POSIXct("1/25/2014", format="%m/%d/%Y"))*1000
      )
    ),
    showlegend = FALSE
  )

fig <- fig %>% 
  add_segments(x = 0, xend = 10000, y = "1/13/2014", yend = "1/13/2014", line = list(dash = "dash"))

fig

```

